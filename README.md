# Explain

This project, conducted at Gachon University during the 3rd year, 2nd semester (2024.6-2024.12), introduces various models tested for comparing similarities between music tracks. The final selected model is the DenseNet121 code, and codes unrelated to model training are included in the TestCode directory. The GitHub repositories and papers referenced are listed below.
For copyright concerns or inquiries, please contact gyuhanhwang@gmail.com.

## Model
- DenseNet: [10]
- TabNet : https://github.com/dreamquark-ai/tabnet
- Transformer : https://github.com/hyunwoongko/transformer

## Dataset
- FMA : https://github.com/mdeff/fma
- Spotify : https://developer.spotify.com


FMA : FMA used the Small dataset. 

Spotify : Spotify created 8 playlists identical to FMA and collected songs from 2017 to October 2024 through URLs.

## Papers

[1] Changheon Song, Yonghyun Lee, Huengjoo Kim, (2020), Improvement of Graph Embedding Based on Siamese Network for Comparison of Music Similarity, KIISE Transactions on Computing Practices,
Vol. 26, No. 11, pp. 493-498, Nov. 2020, doi:10.5626/KTCP.2020.26.11.493

[2] Tsaptsinos, A. (2017). Lyrics-based music genre classification using a hierarchical attention network. *Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)*,
doi:10.48550/arXiv.1707.04678

[3] Zhang, Y., & Duan, Z. (2017). IMINET: Convolutional semi-Siamese networks for sound search by vocal imitation. In *2017 IEEE Workshop on Applications of Signal Processing to Audio and
Acoustics(WASPAA)* (pp. 304-308). New Paltz, NY: IEEE. doi:10.1109/WASPAA.2017.8170026

[4] Andreas Aditya Alvaro Harryanto, Kevin Gunawan, Rio Nagano, Rhio Sutoyo, Music Classification Model Development Based on Audio Recognition using Transformer Model, 2022 3rd INTERNATIONAL
CONFERENCE ON ARTIFICAL INETLLIGENCE AND DATA SCIENCES (AiDAS), Sep.2022, doi:10.1109/AIDAS56890.2022.9918787

[5] Prateek Verma and Jonathan Berger, Audio Transeformers: Transformer Architectures For Large Scale Audio Understanding, Stanford Uni, 2021 IEEE Workshop on Applications of Signal Processing to
Audio Acoustics, May.2021, doi: arXiv:2105.00335vl

[6] Shaojie Bai, J.Zico Kolter, Vladlen Koltun, An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling, Apr.2018, dor:arXiv:1803.01271v2

[7] Mohamed, A., Dahl, G., and Hinton, G.,
“Deep Belief Networks for Phone Recognition,
” Department of Computer Science, University of Toronto, 2009.

[8] He, K., Zhang, X., Ren, S., and Sun, J.,
“Deep Residual Learning for Image Recognition,
” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp.
770-778, doi: 10.1109/CVPR.2016.90.

[9] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger,
“Densely Connected Convolutional Networks,
” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Honolulu, HI, USA, 2017, pp. 4700-4708.

[10] D. T. L. Thuy, T. V. Loan, C. B. Thanh, and N. H. Cuong,
“Music Genre Classification Using DenseNet and Data Augmentation,
” Computer Systems Science & Engineering, vol. 45, no. 1, pp. 123–135,
May 2023. DOI: 10.32604/csse.2023.023473

[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,
“Attention is All You Need,
” in Proc. of Advances in Neural Information Processing Systems
(NeurIPS), Long Beach, CA, USA, Dec. 2017, pp. 5998-6008.

[12] A. Gulati, J. Qin, C.
-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang,
“Conformer: Convolution-augmented Transformer for Speech Recognition,
” in Proc. of IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 5734-5738.

[13] J. Devlin, M.
-W. Chang, K. Lee, and K. Toutanova,
“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,
” in Proc. of North American Chapter of the Association for
Computational Linguistics (NAACL), 2019, pp. 4171-4186.

[14] S. Arik, D. C. M. S. P. K. S., and T. S. K. A. N.,
“TabNet: Attentive Interpretable Tabular Learning,
” in Proc. of the 34th Conference on Neural Information Processing Systems (NeurIPS), 2020, pp. 1-
16.

[15] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson,
“FMA: A dataset for music analysis,
” in Proc. of the 18th International Society for Music Information Retrieval Conference (ISMIR), 2017.

[16] Rafii, Zafar, and Bryan Pardo.
"Music/Voice Separation Using the Similarity Matrix.
" ISMIR. 2012

[17] L. N. M. and S. K. Kopparapu,
“Choice of Mel filter bank in computing MFCC of a resampled speech,
” in Proc. of [conference name, if available], Oct.2014.

[18] 서울대학교 산업공학과 DSBA 연구실 “[Seminar] Mel-frequency cepstrum(MFCC)”
, 2020.06.25.